{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-transformers\n",
    "#!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 372
    },
    "id": "JPk_ijYA3Bc5",
    "outputId": "0e05491e-a86e-42f3-8b6c-4c2cada0c92f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "\n",
    "\n",
    "def to_list(tensor):\n",
    "    return tensor.detach().cpu().tolist()\n",
    " \n",
    "\n",
    "def _get_best_indexes(logits, n_best_size=1):\n",
    "    \"\"\"Get the n-best logits from a list.\"\"\"\n",
    "    index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    best_indexes = []\n",
    "    for i in range(len(index_and_score)):\n",
    "        if i >= n_best_size:\n",
    "            break\n",
    "        best_indexes.append(index_and_score[i][0])\n",
    "    return best_indexes\n",
    " \n",
    "\n",
    "def evaluate(dataset, model, tokenizer):\n",
    "    eval_sampler = SequentialSampler(dataset)\n",
    "    eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=1)\n",
    "\n",
    "    # Eval!\n",
    "    all_results = []\n",
    "    for batch in eval_dataloader:\n",
    "        model.eval()\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        with torch.no_grad():\n",
    "            inputs = {'input_ids':      batch[0],\n",
    "                      'attention_mask': batch[1],\n",
    "                      'token_type_ids': batch[2],\n",
    "                      }\n",
    "            example_indices = batch[3]\n",
    "            outputs = model(**inputs)\n",
    "            start_logits = to_list(outputs[0][0])\n",
    "            end_logits   = to_list(outputs[1][0])\n",
    "            start_indexes = _get_best_indexes(start_logits)\n",
    "            end_indexes = _get_best_indexes(end_logits)\n",
    "    return (start_indexes, end_indexes)\n",
    "  \n",
    "\n",
    "import collections\n",
    "\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "    # Because of the sliding window approach taken to scoring documents, a single\n",
    "    # token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    #\n",
    "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "    # want to consider the score with \"maximum context\", which we define as\n",
    "    # the *minimum* of its left and right context (the *sum* of left and\n",
    "    # right context will always be the same, of course).\n",
    "    #\n",
    "    # In the example the maximum context for 'bought' would be span C since\n",
    "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "    # and 0 right context.\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + 0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index\n",
    "\n",
    "\n",
    "def convert_examples_to_features(tokenizer, question_text, doc_tokens, max_seq_length=384,\n",
    "                                 doc_stride=128, max_query_length=64,\n",
    "                                 cls_token_at_end=False,\n",
    "                                 cls_token='[CLS]', sep_token='[SEP]', pad_token=0,\n",
    "                                 sequence_a_segment_id=0, sequence_b_segment_id=1,\n",
    "                                 cls_token_segment_id=0, pad_token_segment_id=0,\n",
    "                                 mask_padding_with_zero=True):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    query_tokens = tokenizer.tokenize(question_text)\n",
    "    if len(query_tokens) > max_query_length:\n",
    "      query_tokens = query_tokens[0:max_query_length]\n",
    "    tok_to_orig_index = []\n",
    "    orig_to_tok_index = []\n",
    "    all_doc_tokens = []\n",
    "    for (i, token) in enumerate(doc_tokens):\n",
    "        orig_to_tok_index.append(len(all_doc_tokens))\n",
    "        sub_tokens = tokenizer.tokenize(token)\n",
    "        for sub_token in sub_tokens:\n",
    "            tok_to_orig_index.append(i)\n",
    "            all_doc_tokens.append(sub_token)\n",
    "\n",
    "    # The -3 accounts for [CLS], [SEP] and [SEP]\n",
    "    max_tokens_for_doc = max_seq_length - len(query_tokens) - 3\n",
    "\n",
    "    # We can have documents that are longer than the maximum sequence length.\n",
    "    # To deal with this we do a sliding window approach, where we take chunks\n",
    "    # of the up to our max length with a stride of `doc_stride`.\n",
    "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"DocSpan\", [\"start\", \"length\"])\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "        length = len(all_doc_tokens) - start_offset\n",
    "        if length > max_tokens_for_doc:\n",
    "            length = max_tokens_for_doc\n",
    "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "        if start_offset + length == len(all_doc_tokens):\n",
    "            break\n",
    "        start_offset += min(length, doc_stride)\n",
    "\n",
    "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "        tokens = []\n",
    "        token_to_orig_map = {}\n",
    "        token_is_max_context = {}\n",
    "        segment_ids = []\n",
    "\n",
    "        # p_mask: mask with 1 for token than cannot be in the answer (0 for token which can be in an answer)\n",
    "        # Original TF implem also keep the classification token (set to 0) (not sure why...)\n",
    "        p_mask = []\n",
    "\n",
    "        # CLS token at the beginning\n",
    "        if not cls_token_at_end:\n",
    "            tokens.append(cls_token)\n",
    "            segment_ids.append(cls_token_segment_id)\n",
    "            p_mask.append(0)\n",
    "            cls_index = 0\n",
    "\n",
    "        # Query\n",
    "        for token in query_tokens:\n",
    "            tokens.append(token)\n",
    "            segment_ids.append(sequence_a_segment_id)\n",
    "            p_mask.append(1)\n",
    "\n",
    "        # SEP token\n",
    "        tokens.append(sep_token)\n",
    "        segment_ids.append(sequence_a_segment_id)\n",
    "        p_mask.append(1)\n",
    "\n",
    "        # Paragraph\n",
    "        for i in range(doc_span.length):\n",
    "            split_token_index = doc_span.start + i\n",
    "            token_to_orig_map[len(tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "            is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                   split_token_index)\n",
    "            token_is_max_context[len(tokens)] = is_max_context\n",
    "            tokens.append(all_doc_tokens[split_token_index])\n",
    "            segment_ids.append(sequence_b_segment_id)\n",
    "            p_mask.append(0)\n",
    "        paragraph_len = doc_span.length\n",
    "\n",
    "        # SEP token\n",
    "        tokens.append(sep_token)\n",
    "        segment_ids.append(sequence_b_segment_id)\n",
    "        p_mask.append(1)\n",
    "\n",
    "        # CLS token at the end\n",
    "        if cls_token_at_end:\n",
    "            tokens.append(cls_token)\n",
    "            segment_ids.append(cls_token_segment_id)\n",
    "            p_mask.append(0)\n",
    "            cls_index = len(tokens) - 1  # Index of classification token\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        input_mask = [1 if mask_padding_with_zero else 0] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(pad_token)\n",
    "            input_mask.append(0 if mask_padding_with_zero else 1)\n",
    "            segment_ids.append(pad_token_segment_id)\n",
    "            p_mask.append(1)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(input_mask) == max_seq_length\n",
    "        assert len(segment_ids) == max_seq_length\n",
    "    input_ids = torch.tensor([input_ids], dtype=torch.long)\n",
    "    input_mask = torch.tensor([input_mask], dtype=torch.long)\n",
    "    segment_ids = torch.tensor([segment_ids], dtype=torch.long)\n",
    "    cls_index = torch.tensor([cls_index], dtype=torch.long)\n",
    "    p_mask = torch.tensor([p_mask], dtype=torch.float)\n",
    "    example_index = torch.arange(input_ids.size(0), dtype=torch.long)\n",
    "    data = TensorDataset(input_ids, input_mask, segment_ids,\n",
    "                            example_index, cls_index, p_mask)\n",
    "\n",
    "    '''\n",
    "    print(\"*** Example ***\")\n",
    "    \n",
    "    print(\"doc_span_index: %s\" % (doc_span_index))\n",
    "    print('hi')\n",
    "    \n",
    "    print(\"tokens: %s\" % \" \".join(tokens))\n",
    "\n",
    "    print(\"token_to_orig_map: %s\" % \" \".join([\n",
    "                     \"%d:%d\" % (x, y) for (x, y) in token_to_orig_map.items()]))\n",
    "    print(\"token_is_max_context: %s\" % \" \".join([\n",
    "                     \"%d:%s\" % (x, y) for (x, y) in token_is_max_context.items()\n",
    "                 ]))\n",
    "    print(\"input_ids: %s\" % \" \".join([str(x) for x in input_ids]))\n",
    "    print(\"input_mask: %s\" % \" \".join([str(x) for x in input_mask]))\n",
    "    print(\"segment_ids: %s\" % \" \".join([str(x) for x in segment_ids]))\n",
    "    '''\n",
    "    \n",
    "    return data, tokens\n",
    "\n",
    "# 因為要我們今天要跑的是中文QA 所以只有Bert可以用\n",
    "import torch\n",
    "from pytorch_transformers import (WEIGHTS_NAME, BertConfig,\n",
    "                                  BertForQuestionAnswering, BertTokenizer)\n",
    "\n",
    " \n",
    "device = torch.device('cuda')\n",
    "print(device)\n",
    "\n",
    "checkpoint = 'bert-chinese-qa'\n",
    "from transformers import AutoModelForQuestionAnswering, BertTokenizer\n",
    "\n",
    "\n",
    "config_class, model_class, tokenizer_class = BertConfig, BertForQuestionAnswering, BertTokenizer\n",
    "model = AutoModelForQuestionAnswering.from_pretrained('wptoux/albert-chinese-large-qa').to(device)\n",
    "tokenizer = BertTokenizer.from_pretrained('wptoux/albert-chinese-large-qa', do_lower_case=True)\n",
    "\n",
    "#model = model_class.from_pretrained(checkpoint).to(device)\n",
    "#tokenizer = tokenizer_class.from_pretrained('bert-base-chinese', do_lower_case=True)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'清華大學成立於民國前一年'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context = '清華大學成立於民國前一年，校址為北平西郊的清華園，最初名稱為「清華學堂」。105年11月1日起，國立清華大學與國立新竹教育大學合併為「國立清華大學」。原「國立新竹教育大學附設實驗國民小學」同時更名為「國立清華大學附設實驗國民小學」。'\n",
    "question = '清大何時成立'\n",
    "data, tokens = convert_examples_to_features(tokenizer=tokenizer, question_text=question, doc_tokens=context)\n",
    "start, end = evaluate(data, model, tokenizer)\n",
    "\"\".join(tokens[start[0]: end[0]+1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'清華學堂'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '清華大學最初叫什麼'\n",
    "data, tokens = convert_examples_to_features(tokenizer=tokenizer, question_text=question, doc_tokens=context)\n",
    "start, end = evaluate(data, model, tokenizer)\n",
    "\"\".join(tokens[start[0]: end[0]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'國立新竹教育大學'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = '清華大學與哪間學校合併'\n",
    "data, tokens = convert_examples_to_features(tokenizer=tokenizer, question_text=question, doc_tokens=context)\n",
    "start, end = evaluate(data, model, tokenizer)\n",
    "\"\".join(tokens[start[0]: end[0]+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "collapsed_sections": [],
   "name": "QA.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('py36': conda)",
   "language": "python",
   "name": "python3711jvsc74a57bd082f90a6d6086172460f4c9c1e1a219fdde808fd64cbe4fc0c473da247502f162"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
